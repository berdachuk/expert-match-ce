spring:
  application:
    name: expert-match
    version: 1.0.0

  # Spring AI auto-configuration exclusion
  # NOTE: We ALWAYS use custom Java configuration (SpringAIConfig) to create ChatModel and EmbeddingModel beans
  # with separate base URLs. All Spring AI auto-configurations are disabled for ALL profiles.
  autoconfigure:
    exclude:
      # Exclude ALL OpenAI auto-configurations
      - org.springframework.ai.model.openai.autoconfigure.OpenAiAudioSpeechAutoConfiguration
      - org.springframework.ai.model.openai.autoconfigure.OpenAiAudioTranscriptionAutoConfiguration
      - org.springframework.ai.model.openai.autoconfigure.OpenAiImageAutoConfiguration
      - org.springframework.ai.model.openai.autoconfigure.OpenAiImageModelAutoConfiguration
      - org.springframework.ai.model.openai.autoconfigure.OpenAiModerationAutoConfiguration
      - org.springframework.ai.model.openai.autoconfigure.OpenAiChatAutoConfiguration
      - org.springframework.ai.model.openai.autoconfigure.OpenAiEmbeddingAutoConfiguration
      # Exclude ALL Ollama auto-configurations
      - org.springframework.ai.model.ollama.autoconfigure.OllamaChatAutoConfiguration
      - org.springframework.ai.model.ollama.autoconfigure.OllamaEmbeddingAutoConfiguration
      - org.springframework.ai.model.ollama.autoconfigure.OllamaApiAutoConfiguration

  # Database Configuration
  datasource:
    url: ${SPRING_DATASOURCE_URL:jdbc:postgresql://localhost:5433/expertmatch}
    username: ${SPRING_DATASOURCE_USERNAME:${DB_USERNAME:expertmatch}}
    password: ${SPRING_DATASOURCE_PASSWORD:${DB_PASSWORD:expertmatch}}
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 10
      minimum-idle: 5
      connection-timeout: 30000
      # Set search_path to include public schema so vector type is accessible
      connection-init-sql: SET search_path = public, expertmatch;

  # JDBC Configuration
  jdbc:
    template:
      fetch-size: 100

  # Jackson Configuration
  jackson:
    time-zone: Europe/Minsk  # Set timezone to Minsk (UTC+3)

  # Flyway Configuration
  flyway:
    baseline-on-migrate: true
    baseline-version: 0
    enabled: true
    schemas: expertmatch
    default-schema: expertmatch

  # Security configuration
  # Note: Authentication and authorization are handled by Spring Gateway.
  # Spring Gateway validates JWT tokens and populates user information in HTTP headers:
  # - X-User-Id: User identifier
  # - X-User-Roles: Comma-separated list of roles (e.g., "ROLE_USER,ROLE_ADMIN")
  # - X-User-Email: User email address

  # Spring Modulith Configuration
  modulith:
    # Exclude generated API code from module detection
    # The api module contains generated OpenAPI client code in target/generated-sources
    # and should not be treated as a separate module by Spring Modulith
    additional-packages:
      - "!com.berdachuk.expertmatch.api"  # Exclude generated api module

  # Spring AI Configuration
  # NOTE: Auto-configuration is DISABLED for ALL profiles - all models are created via SpringAIConfig.java
  # You can configure different providers for different tasks:
  # 1. LLM Inference (Chat) - for query processing and answer generation
  # 2. Embedding - for vector embeddings
  # 3. Reranking - for semantic reranking (optional)
  #
  # Provider Selection:
  # - Set LLM_PROVIDER=ollama or LLM_PROVIDER=openai for chat/LLM
  # - Set EMBEDDING_PROVIDER=ollama or EMBEDDING_PROVIDER=openai for embeddings
  # - Set RERANKING_PROVIDER=ollama or RERANKING_PROVIDER=openai for reranking
  #
  # All models are created via SpringAIConfig.java custom configuration.
  # Configure the providers you need - you can mix Ollama and OpenAI for different tasks.
  ai:
    # Disable Spring AI auto-configuration - we use custom configuration only
    ollama:
      enabled: false
      # Ollama Provider (local or remote)
      # NOTE: These properties are kept for reference but won't be used by auto-configuration
      # They are used by SpringAIConfig.java custom configuration instead
      base-url: ${OLLAMA_BASE_URL:http://localhost:11435}
      chat:
        options:
          # Default local LLM model: qwen3:4b-instruct-2507-q4_K_M
          # Alternative: qwen3:30b-a3b-instruct-2507-q4_K_M (use OLLAMA_MODEL env var)
          # Override with OLLAMA_MODEL environment variable if needed
          model: ${OLLAMA_MODEL:qwen3:4b-instruct-2507-q4_K_M}
          temperature: 0.7
      embedding:
        embedding:
          options:
            model: ${OLLAMA_EMBEDDING_MODEL:qwen3-embedding-8b}
      # Reranking support (if using Ollama for reranking)
      # Note: Reranking typically uses a separate reranker model
      # Available models: dengcao/Qwen3-Reranker-8B:Q4_K_M (recommended)
      #                   dengcao/Qwen3-Reranker-0.6B:Q8_0 (lightweight)
      # Pull with: ollama pull dengcao/Qwen3-Reranker-8B:Q4_K_M
      reranking:
        options:
          model: ${OLLAMA_RERANKING_MODEL:dengcao/Qwen3-Reranker-8B:Q4_K_M}

    # OpenAI or OpenAI-compatible provider configuration (e.g., OpenAI, Azure OpenAI, Anthropic Claude, etc.)
    # Note: Ollama Cloud (https://ollama.com/api) uses Ollama's native API format, not OpenAI-compatible format.
    # For OpenAI-compatible providers, use providers like OpenAI, Azure OpenAI, or other compatible services.
    # NOTE: enabled: false is set below - these properties are kept for reference but won't be used by auto-configuration
    # They are used by SpringAIConfig.java custom configuration instead
    openai:
      enabled: false
      # API key for OpenAI or OpenAI-compatible providers
      # Note: For different providers per task, use task-specific keys (LLM_OPENAI_API_KEY, EMBEDDING_OPENAI_API_KEY)
      # Spring AI uses the same api-key and base-url for both chat and embedding by default.
      # To use different providers, configure only the sections you need or use Spring profiles.
      # For separate base URLs per component, use spring.ai.custom.* properties below.
      api-key: ${LLM_OPENAI_API_KEY:${EMBEDDING_OPENAI_API_KEY:${OPENAI_API_KEY:}}}
      # Custom base URL for OpenAI-compatible providers
      # Examples:
      # - OpenAI: Leave empty (defaults to https://api.openai.com)
      # - Azure OpenAI: https://YOUR_RESOURCE.openai.azure.com
      # - Other compatible providers: https://api.provider.com/v1
      # Note: For different providers per task, use task-specific URLs (LLM_OPENAI_BASE_URL, EMBEDDING_OPENAI_BASE_URL)
      #       or configure separate base URLs via spring.ai.custom.* properties below.
      # Leave empty to use default OpenAI API endpoint (https://api.openai.com)
      base-url: ${LLM_OPENAI_BASE_URL:${EMBEDDING_OPENAI_BASE_URL:${OPENAI_BASE_URL:}}}
      chat:
        options:
          # Model name for chat (e.g., gpt-4, gpt-3.5-turbo, or provider-specific model names)
          # Can be overridden with LLM_OPENAI_CHAT_MODEL for LLM-specific configuration
          model: ${LLM_OPENAI_CHAT_MODEL:${OPENAI_CHAT_MODEL:gpt-4-turbo-preview}}
          temperature: ${LLM_OPENAI_TEMPERATURE:${OPENAI_TEMPERATURE:0.7}}
          max-tokens: ${LLM_OPENAI_MAX_TOKENS:${OPENAI_MAX_TOKENS:4096}}
      embedding:
        options:
          # Embedding model (e.g., text-embedding-3-large, text-embedding-ada-002)
          # For OpenAI-compatible providers, use their model names
          # Can be overridden with EMBEDDING_OPENAI_MODEL for embedding-specific configuration
          model: ${EMBEDDING_OPENAI_MODEL:${OPENAI_EMBEDDING_MODEL:text-embedding-3-large}}
          # Embedding dimensions (1536 for text-embedding-3-large, 3072 for full dimensions)
          # Can be overridden with EMBEDDING_OPENAI_DIMENSIONS for embedding-specific configuration
          dimensions: ${EMBEDDING_OPENAI_DIMENSIONS:${OPENAI_EMBEDDING_DIMENSIONS:1536}}

    # Custom configuration for separate base URLs and providers per component
    # These properties are used by SpringAIConfig.java to create separate beans
    # for chat, embedding, and reranking with different base URLs and providers
    # If not configured, falls back to spring.ai.openai.* or spring.ai.ollama.* auto-configuration
    custom:
      chat:
        # Provider selection: 'ollama' or 'openai' (default: 'openai')
        # Determines which AI provider to use for chat/LLM service
        provider: ${CHAT_PROVIDER:${LLM_PROVIDER:openai}}
        # Base URL for chat/LLM service
        # For Ollama: http://localhost:11434 (native Ollama API)
        # For OpenAI-compatible: https://api.openai.com, https://YOUR_RESOURCE.openai.azure.com, etc.
        base-url: ${CHAT_BASE_URL:${LLM_OPENAI_BASE_URL:${OPENAI_BASE_URL:}}}
        # API key for chat service (required for OpenAI-compatible providers, optional for Ollama)
        api-key: ${CHAT_API_KEY:${LLM_OPENAI_API_KEY:${OPENAI_API_KEY:}}}
        # Chat model name
        # For Ollama: qwen3:4b-instruct-2507-q4_K_M, qwen3:30b-a3b-instruct-2507-q4_K_M, etc.
        # For OpenAI: gpt-4, gpt-3.5-turbo, gpt-4-turbo-preview, etc.
        model: ${CHAT_MODEL:${LLM_OPENAI_CHAT_MODEL:${OPENAI_CHAT_MODEL:gpt-4-turbo-preview}}}
        # Chat temperature
        temperature: ${CHAT_TEMPERATURE:${LLM_OPENAI_TEMPERATURE:${OPENAI_TEMPERATURE:0.7}}}
      embedding:
        # Provider selection: 'ollama' or 'openai' (default: 'openai')
        # Determines which AI provider to use for embedding service
        provider: ${EMBEDDING_PROVIDER:${EMBEDDING_OPENAI_PROVIDER:openai}}
        # Base URL for embedding service
        # For Ollama: http://localhost:11434 (native Ollama API)
        # For OpenAI-compatible: https://api.openai.com, https://YOUR_RESOURCE.openai.azure.com, etc.
        base-url: ${EMBEDDING_BASE_URL:${EMBEDDING_OPENAI_BASE_URL:${OPENAI_BASE_URL:}}}
        # API key for embedding service (required for OpenAI-compatible providers, optional for Ollama)
        api-key: ${EMBEDDING_API_KEY:${EMBEDDING_OPENAI_API_KEY:${OPENAI_API_KEY:}}}
        # Embedding model name
        # For Ollama: qwen3-embedding:8b, qwen3-embedding:0.6b, etc.
        # For OpenAI: text-embedding-3-large, text-embedding-ada-002, etc.
        model: ${EMBEDDING_MODEL:${EMBEDDING_OPENAI_MODEL:${OPENAI_EMBEDDING_MODEL:text-embedding-3-large}}}
        # Embedding dimensions (for OpenAI models only, Ollama models have fixed dimensions)
        dimensions: ${EMBEDDING_DIMENSIONS:${EMBEDDING_OPENAI_DIMENSIONS:${OPENAI_EMBEDDING_DIMENSIONS:1536}}}
      reranking:
        # Provider selection: 'ollama' or 'openai' (default: 'ollama')
        # Determines which AI provider to use for reranking service
        provider: ${RERANKING_PROVIDER:${RERANKING_OPENAI_PROVIDER:ollama}}
        # Base URL for reranking service
        # For Ollama: http://localhost:11434 (native Ollama API)
        # For OpenAI-compatible: https://api.openai.com, https://YOUR_RESOURCE.openai.azure.com, etc.
        base-url: ${RERANKING_BASE_URL:${RERANKING_OPENAI_BASE_URL:${OLLAMA_BASE_URL:http://localhost:11435}}}
        # API key for reranking service (required for OpenAI-compatible providers, optional for Ollama)
        api-key: ${RERANKING_API_KEY:${RERANKING_OPENAI_API_KEY:${OPENAI_API_KEY:}}}
        # Reranking model name
        # For Ollama: dengcao/Qwen3-Reranker-8B:Q4_K_M, dengcao/Qwen3-Reranker-0.6B:Q8_0, etc.
        # For OpenAI: typically uses a chat model like gpt-4, gpt-3.5-turbo, etc.
        model: ${RERANKING_MODEL:${RERANKING_OPENAI_MODEL:${OLLAMA_RERANKING_MODEL:dengcao/Qwen3-Reranker-8B:Q4_K_M}}}
        # Reranking temperature (lower is better for reranking, default: 0.1)
        temperature: ${RERANKING_TEMPERATURE:${RERANKING_OPENAI_TEMPERATURE:0.1}}

    # Note: To use different providers for different tasks, configure them separately:
    #
    # For LLM Inference (Chat):
    # - Set LLM_PROVIDER=ollama to use Ollama (configure spring.ai.ollama.chat.*)
    # - Set LLM_PROVIDER=openai to use OpenAI (configure spring.ai.openai.chat.*)
    # - Use LLM_OPENAI_API_KEY, LLM_OPENAI_BASE_URL, LLM_OPENAI_CHAT_MODEL for OpenAI-specific LLM config
    # - For separate base URL: Set CHAT_BASE_URL, CHAT_API_KEY, CHAT_MODEL (uses spring.ai.custom.chat.*)
    #
    # For Embeddings:
    # - Set EMBEDDING_PROVIDER=ollama to use Ollama (configure spring.ai.ollama.embedding.*)
    # - Set EMBEDDING_PROVIDER=openai to use OpenAI (configure spring.ai.openai.embedding.*)
    # - Use EMBEDDING_OPENAI_API_KEY, EMBEDDING_OPENAI_BASE_URL, EMBEDDING_OPENAI_MODEL for OpenAI-specific embedding config
    # - For separate base URL: Set EMBEDDING_BASE_URL, EMBEDDING_API_KEY, EMBEDDING_MODEL (uses spring.ai.custom.embedding.*)
    #
    # For Reranking:
    # - Set RERANKING_PROVIDER=ollama to use Ollama (configure expertmatch.retrieval.reranking.model)
    # - Set RERANKING_PROVIDER=openai to use OpenAI (requires custom implementation)
    # - Use RERANKING_OPENAI_API_KEY, RERANKING_OPENAI_BASE_URL, RERANKING_OPENAI_MODEL for OpenAI-specific reranking config
    # - For separate base URL: Set RERANKING_BASE_URL, RERANKING_MODEL (uses spring.ai.custom.reranking.*)
    #
    # Spring AI will auto-configure ChatModel and EmbeddingModel based on what's configured.
    # If both Ollama and OpenAI are configured, Spring AI may create both beans - use Spring profiles
    # or conditional configuration to select which one to use for each task.
    #
    # Separate Base URLs and Providers:
    # - Configure spring.ai.custom.* properties to use different providers and services
    #   for chat, embedding, and reranking components
    # - Each component can use a different provider (ollama or openai) and base URL
    # - Example: Use OpenAI for chat, Ollama for embedding, Ollama for reranking
    #   export CHAT_PROVIDER=openai
    #   export CHAT_BASE_URL=https://api.openai.com
    #   export CHAT_API_KEY=sk-...
    #   export CHAT_MODEL=gpt-4
    #   export EMBEDDING_PROVIDER=ollama
    #   export EMBEDDING_BASE_URL=http://localhost:11434
    #   export EMBEDDING_MODEL=qwen3-embedding:8b
    #   export RERANKING_PROVIDER=ollama
    #   export RERANKING_BASE_URL=http://localhost:11434
    #   export RERANKING_MODEL=dengcao/Qwen3-Reranker-8B:Q4_K_M

    vectorstore:
      pgvector:
        index-type: HNSW
        distance-type: COSINE_DISTANCE

# Server Configuration
server:
  port: ${SERVER_PORT:8080}
  error:
    include-message: always
    include-binding-errors: always

# Actuator
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics
  endpoint:
    health:
      show-details: always
      show-components: always

# OpenAPI Documentation
springdoc:
  api-docs:
    path: /api/v1/openapi.json
  swagger-ui:
    path: /swagger-ui.html
    enabled: true
  default-consumes-media-type: application/json
  default-produces-media-type: application/json

# Application Configuration
expertmatch:
  # Security header configuration
  # Headers populated by Spring Gateway after authentication/authorization
  security:
    headers:
      user-id: ${EXPERTMATCH_SECURITY_HEADER_USER_ID:X-User-Id}
      user-roles: ${EXPERTMATCH_SECURITY_HEADER_USER_ROLES:X-User-Roles}
      user-email: ${EXPERTMATCH_SECURITY_HEADER_USER_EMAIL:X-User-Email}
  # External system configuration
  external:
    # Person profile URL template for linking to external system expert profiles
    # Use {externalId} placeholder for the expert's external ID
    # Example: https://people.example.com/profile/{externalId}
    person-profile-url-template: ${EXPERTMATCH_EXTERNAL_PERSON_PROFILE_URL_TEMPLATE:}
  ingestion:
    constant-expansion:
      enabled: false  # Disable LLM constant expansion by default (enable in local/dev profiles)
      # When enabled, ConstantExpansionService expands domain-specific constants
      # (technologies, tools, project types, etc.) using LLM at application startup.
      # Expansion results are cached to avoid repeated LLM calls.
      # Falls back to base constants if LLM is unavailable or fails.
    external-database:
      # External database configuration for work experience ingestion
      enabled: ${EXPERTMATCH_INGESTION_EXTERNAL_DB_ENABLED:false}
      host: ${INGEST_POSTGRES_HOST:localhost}
      port: ${INGEST_POSTGRES_PORT:}
      database: ${INGEST_POSTGRES_DB:}
      username: ${INGEST_POSTGRES_USER:}
      password: ${INGEST_POSTGRES_PASSWORD:}
      schema: ${INGEST_POSTGRES_SCHEMA:}
      connection-timeout: ${EXPERTMATCH_INGESTION_EXTERNAL_DB_CONNECTION_TIMEOUT:30000}
      maximum-pool-size: ${EXPERTMATCH_INGESTION_EXTERNAL_DB_MAX_POOL_SIZE:5}
      minimum-idle: ${EXPERTMATCH_INGESTION_EXTERNAL_DB_MIN_IDLE:2}
  mcp:
    server:
      enabled: true
      base-url: /mcp
  chat:
    default-chat:
      enabled: true
      name-pattern: "Default Chat"
    history:
      # Maximum tokens for conversation history in context
      max-tokens: ${EXPERTMATCH_CHAT_HISTORY_MAX_TOKENS:2000}
      # Maximum number of recent messages to include in context
      max-messages: ${EXPERTMATCH_CHAT_HISTORY_MAX_MESSAGES:10}
      # Token threshold for triggering summarization (when history exceeds this, summarize older messages)
      summarization-threshold-tokens: ${EXPERTMATCH_CHAT_HISTORY_SUMMARIZATION_THRESHOLD:1500}
      # Maximum tokens for summarized context
      max-summary-tokens: ${EXPERTMATCH_CHAT_HISTORY_MAX_SUMMARY_TOKENS:500}
  retrieval:
    vector:
      max-results: 100
      similarity-threshold: 0.7
    graph:
      max-depth: 3
    reranking:
      enabled: true
      # Provider selection: 'ollama' or 'openai' (for future OpenAI reranking support)
      provider: ${RERANKING_PROVIDER:ollama}
      # Model name (provider-specific)
      # For Ollama: dengcao/Qwen3-Reranker-8B:Q4_K_M (recommended), dengcao/Qwen3-Reranker-0.6B:Q8_0 (lightweight)
      #             Format: dengcao/Qwen3-Reranker-{SIZE}:{QUANTIZATION}
      #             Pull with: ollama pull dengcao/Qwen3-Reranker-8B:Q4_K_M
      # For OpenAI: typically uses a chat model for relevance scoring
      model: ${RERANKING_MODEL:dengcao/Qwen3-Reranker-8B:Q4_K_M}
      # OpenAI-specific reranking configuration (if using OpenAI for reranking)
      openai:
        api-key: ${RERANKING_OPENAI_API_KEY:${OPENAI_API_KEY:}}
        base-url: ${RERANKING_OPENAI_BASE_URL:${OPENAI_BASE_URL:}}
        model: ${RERANKING_OPENAI_MODEL:${OPENAI_CHAT_MODEL:gpt-4-turbo-preview}}
        temperature: ${RERANKING_OPENAI_TEMPERATURE:0.1}  # Lower temperature for reranking
  llm:
    max-tokens: 4096
    temperature: 0.7
  sgr:
    enabled: true
    schema-validation: true  # Enable JSON Schema validation
    cascade:
      enabled: true
      model: ${LLM_MODEL:}  # Use same model as chat (optional override)
    routing:
      enabled: true
      model: ${LLM_MODEL:}  # Use same model as chat (optional override)
    cycle:
      enabled: true
      model: ${LLM_MODEL:}  # Use same model as chat (optional override)
  tools:
    search:
      enabled: ${EXPERTMATCH_TOOLS_SEARCH_ENABLED:false}  # Disabled: Incompatible with Spring AI 1.1.0 (ToolCallAdvisor is final) and conflicts with Agent Skills
      strategy: ${EXPERTMATCH_TOOLS_SEARCH_STRATEGY:pgvector}  # pgvector, lucene, regex
      max-results: ${EXPERTMATCH_TOOLS_SEARCH_MAX_RESULTS:5}  # Maximum tools to return per search
  skills:
    enabled: ${EXPERTMATCH_SKILLS_ENABLED:false}  # Enable Agent Skills (disabled by default)
    directory: ${EXPERTMATCH_SKILLS_DIRECTORY:.claude/skills}  # Skills directory (classpath or filesystem)

# Logging
logging:
  level:
    com.berdachuk.expertmatch: INFO
    com.berdachuk.expertmatch.system: INFO  # Log health checks
    org.springframework.modulith: INFO
    org.springframework.ai: INFO
    org.springframework.web: INFO
    org.springframework.security: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
  file:
    name: logs/expert-match.log
    max-size: 10MB
    max-history: 30

# To enable debug logging, activate the debug profile:
# --spring.profiles.active=local,debug
# This will override the default logging levels with DEBUG/TRACE levels

